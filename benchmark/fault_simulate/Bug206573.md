这是一个关于 **NFS `fsc` (fscache) 缓存一致性**的复杂问题，涉及内核版本变更带来的行为差异、缓存键（Cache Key）生成机制以及 NFSv3 和 NFSv4 的不同实现。我们来深入分析这个现象的根源、原因和可能的解决方案。
[链接](https://bugzilla.kernel.org/show_bug.cgi?id=206573)
---

## 🐞 现象总结

1.  **NFSv3 + `fsc`**:
    *   首次挂载并访问文件，数据被缓存到 `/var/cache/fscache/cache/.../@02`。
    *   卸载后重新挂载**相同的 NFSv3 共享**。
    *   再次访问相同文件，**没有使用之前的缓存**，而是创建了一个**新的缓存条目** `@09`，并重新下载数据。
    *   **关键点**：即使指定 `fsc=<tag>`，服务器 `fsid` 和 ID 也未改变。
    *   **例外**：如果客户端上**始终有另一个到同一服务器的连接**（即使不同路径），缓存键保持一致。

2.  **NFSv4 + `fsc`**:
    *   问题更严重：即使不重新挂载，**持续读取同一个文件也会导致缓存被反复写入，而不会从缓存中读取**。缓存完全失效。

3.  **版本差异**:
    *   **工作**：Linux 内核 ~4.15, RHEL7
    *   **不工作**：Linux 内核 ~4.17+, RHEL8

---

## 🔍 一、核心问题：缓存键（Cache Key）的生成

`fscache` 为每个缓存对象生成一个唯一的键。这个键决定了数据存储在磁盘缓存的哪个位置。如果键变了，即使数据相同，也会被视为“新”对象。

### 1. NFSv3 缓存键的构成

理论上，`fscache` 为 NFSv3 生成缓存键时，应基于：
*   服务器地址
*   文件系统 ID (`fsid`)
*   文件的 `inode` 号
*   可能还包括一些客户端内部的、与连接相关的标识符

### 2. Bug 的根源：连接上下文的丢失

根据你的描述和已知的内核变更，问题很可能出在：

> **从某个内核版本（~4.16/4.17）开始，`fscache` 在生成 NFSv3 缓存键时，引入了一个与** **NFSv3 本身是** **无状态** **的，但 `fscache` 为了管理缓存，需要一个“连接上下文”或“会话标识”。**

*   **旧内核 (<=4.15)**: 可能使用了一个更稳定的、基于服务器和 `fsid` 的标识符，或者在连接重建时能复用之前的上下文。
*   **新内核 (>=4.17)**: 实现可能改变，导致每次 `mount` 都创建了一个**新的、唯一的连接上下文**，这个上下文被纳入了缓存键的计算。
*   **为什么“另一个连接存在”能解决？**: 当客户端上还有另一个到同一 NFS 服务器的连接时，`fscache` 子系统可能认为“到该服务器的连接已经建立”，因此为新的挂载复用相同的上下文或标识符，从而保持了缓存键的一致性。

### 3. 证据：`@02` vs `@09`

你观察到的缓存目录从 `@02` 变为 `@09`，这正是缓存键改变的直接体现。`@02` 和 `@09` 是同一个缓存对象在不同“命名空间”或“上下文”下的不同实例。

---

## 🔍 二、NFSv4 的问题：更深层次的失效

NFSv4 的问题（持续写入，从不读取）比 NFSv3 的“键变化”更严重，表明 `fscache` 的集成存在更根本的缺陷。

### 1. NFSv4 的有状态性

NFSv4 是有状态协议，有“会话”(Session) 的概念。`fscache` 理论上应该能更好地与之集成。

### 2. 可能的原因

*   **缓存验证失败**: `fscache` 可能每次访问都尝试验证缓存的有效性，但由于 NFSv4 的某些属性（如 `change_attr`）更新过于频繁或 `fscache` 的验证逻辑过于严格，导致它总是认为缓存“过期”，从而强制重新下载。
*   **`open`/`close` 语义**: NFSv4 的 `open` 操作可能触发了 `fscache` 的“写回”或“失效”逻辑，即使只是读操作。
*   **内核集成 Bug**: `fs/nfs/fscache.c` 中的代码可能存在一个循环：读取文件 -> `fscache` 标记为脏或失效 -> 下次读取强制回源 -> 重复。这与你观察到的“持续写入”相符。
*   **版本差异**: RHEL8 使用的内核版本和 `fscache` 实现可能与 RHEL7 有显著不同，引入了这个回归。

---

## 🛠️ 三、解决方案与建议

### ✅ 1. 验证和报告

*   **确认内核版本**: 明确哪个内核版本是分水岭。尝试在 4.15 和 4.17 之间更精细地测试。
*   **搜索已知 Bug**: 在 Linux 内核邮件列表 (LKML)、Bugzilla (如 Red Hat Bugzilla) 中搜索关键词：
    *   `fscache nfs remount key`
    *   `fscache nfs connection context`
    *   `fscache nfs4 always write`
*   **提交 Bug 报告**: 如果未找到，向 Linux 内核 NFS 子系统或 `fscache` 维护者提交详细的 bug 报告，包含你的测试步骤、内核版本、`dmesg` 日志（启用 `fscache` 调试信息）。

### ⚠️ 2. Workaround (NFSv3)

*   **避免重新挂载**: 尽可能保持 NFS 共享挂载状态。如果需要“刷新”，考虑使用 `mount -o remount`（这通常不会重建连接上下文）或 `sync`。
*   **维持“锚定”连接**: 如你发现的，在客户端上**始终挂载一个到该 NFS 服务器的、不常访问的共享**（例如 `server:/export` 到 `/mnt/anchor`）。这可以“固定”连接上下文，确保其他共享的缓存键稳定。
*   **降级内核**: 如果可行且允许，使用已知工作正常的内核版本（如 4.15 或 RHEL7 内核）。

### ❌ 3. Workaround (NFSv4)

*   **禁用 `fsc`**: 对于 NFSv4，目前看来 `fsc` 可能完全不可用。最实际的方案是**不要使用 `fsc` 选项**。
*   **依赖内核页缓存**: 接受 NFSv4 的性能依赖于内核的 `page cache`，它虽然易失，但在单次会话中有效。

### 🛠️ 4. 技术探索

*   **`fscache` 调试**: 启用 `fscache` 的调试信息，观察 `dmesg` 中的缓存操作（获取、释放、读取、写入、失效）。
    ```bash
    # 可能需要重新编译内核或加载模块时指定参数
    echo 1 > /proc/sys/fs/fscache/debug
    ```
*   **检查 `cachefilesd`**: 确保 `cachefilesd` 版本和配置正确。虽然问题更可能在内核，但 `cachefilesd` 的行为也需确认。
*   **NFS 挂载选项**: 尝试添加 `lookupcache=pos` 或调整 `actimeo`，看是否影响 `fscache` 的行为（可能性较低，但值得尝试）。

---

## 📊 四、总结

| 问题 | 根本原因 | 解决方案 |
| :--- | :--- | :--- |
| **NFSv3 缓存键在 remount 后改变** | 内核变更导致 `fscache` 为每次 `mount` 创建新的连接上下文，并将其纳入缓存键计算。 | 1. 避免重新挂载。<br>2. 维持一个到服务器的“锚定”连接。<br>3. 降级内核或等待修复。 |
| **NFSv4 缓存持续写入，从不读取** | `fscache` 与 NFSv4 集成存在严重 Bug，可能导致缓存验证逻辑错误或状态管理混乱。 | **禁用 `fsc`**。依赖内核 `page cache`。这是一个已知的、影响 RHEL8 等较新系统的严重回归。 |

---

> 📢 **最终结论**：你遇到的不是一个配置问题，而是一个**内核级别的回归 Bug**。NFSv3 的问题源于缓存键生成逻辑对“连接上下文”的依赖增强，而 NFSv4 的问题则更为严重，表明 `fscache` 功能在现代内核中对于 NFSv4 已经基本失效。**最安全的建议是：在 RHEL8 / 内核 4.17+ 环境中，避免对 NFSv4 使用 `fsc`，并对 NFSv3 的重新挂载保持警惕，或采用“锚定连接”作为 Workaround。** 这凸显了 `fscache` 作为一个相对小众且复杂的子系统，在维护和向后兼容性上面临的挑战。